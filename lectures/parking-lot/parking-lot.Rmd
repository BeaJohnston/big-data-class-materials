---
# Example 2: The Causal Effect of Neighborhoods

- Fertilizer is relatively easy to randomize and run an RCT

- But most policy questions are not so easy to answer with an RCT

- Why?

  - "Noisy" economic outcomes require larger sample sizes, but it is infeasible to randomize at scale
  - Also, unethical to randomize certain types of characteristics

- When we cannot experiment happens, we look for .hi[quasi-experimental] designs

---
# Quasi-experimental designs

- Quasi-experimental designs are not true experiments, but they can be used to estimate causal effects

- Key idea: .hi[exploit] some kind of .hi[exogenous] variation in the explanatory variable of interest

  - If the variation is exogenous, then we can use it to estimate causal effects
  - Why? It is uncorrelated with the error term in the regression
  - We have .hi[as good as] random assignment because the assignment is unrelated to other factors that affect the outcome
  - Intuition: 

- Sometimes the term "natural" experiments is used to introduce a quasi-experimental design
  - This means that the as good as random assignment was created by "nature" or oftentimes, policy

---
# Quasi-experiments and assumptions

- Every quasi-experimental design requires an assumption 
- Leads to contentious debates over their validity

--

Common Examples:
  - **Difference-in-differences**: Compare outcomes in units that do and do not experience a treatment, before and after the treatment
    - Compare employment outcomes in states that change and do not change minimum wage (Card and Krueger (1993))
  - **Regression Discontinuity Design**: Compare outcomes for units just above and just below some cutoff that determines a treatment
    - Compare economic outcomes for students just above and just below GPA cutoff to be admitted to college (Zimmerman (2014))
  - **Instrumental variables**: Take a variable that moves the explanatory variable but is uncorrelated with the error term
    - Proximity of univerisities -> increased educational attainment -> higher earnings (Card (1995))

---
# Causality of Neighborhoods vs. Sorting

- Two very different explanations for variation in children's outcomes across areas

  1. Sorting: different people live in different places

  2. Causal effects: places have a causal effect on upward mobility for a given person

---
# Causal Effects of Neighborhoods

- Ideal experiment: randomly assign children to neighborhoods and compare outcomes in adulthood
  - Any issues with this?

- How can we approximate this same thing?

--

- Chetty and Hendren (2018) use a .hi[quasi-experimental] design:

  - Sample of 3 million families that move across Census tracts

  - Key idea: exploit variation in the _age of child_ when the family moves to identify causal effects of neighborhood


---
# Moving a short distance in Boston

<figure>
    <img src="pics/roxbury_to_savin_oppatlas.png" height="50%"
         alt="Roxbury to Savin Hill in Opportunity Atlas">
    <figcaption>Opportunity Atlas of MA: Savin Hill outlined, Roxbury nextdoor.</figcaption>
</figure>

---
## Moving to a Higher Mobility Area and Income 

<figure>
    <img src="./pics/blank_income_gain.png" height="50%"
         alt="Blank graph space with income gain on y-axis and age of move on x-axis">
    <figcaption>Chetty and Hendren (2018).</figcaption>
</figure>

---
## Moving to a Higher Mobility Area and Income

<figure>
    <img src="./pics/first_data_point_income_gain.png" height="50%"
         alt="First data point on graph with income gain on y-axis and age of move on x-axis. Data point is for moving at age 2 from Roxbury to Savin Hill leads to average earnings of $38K">
    <figcaption>Chetty and Hendren (2018).</figcaption>
</figure>

---
## Moving to a Higher Mobility Area and Income

<figure>
    <img src="./pics/all_data_points_income_gain.png" height="50%"
         alt="All data points on graph with income gain on y-axis and age of move on x-axis for ages up to 23 with a downward slope from $38K to $23K.">
    <figcaption>Chetty and Hendren (2018).</figcaption>
</figure>

---
## Moving to a Higher Mobility Area and Income

<figure>
    <img src="./pics/level_off_data_points_income_gain.png" height="50%"
         alt="All data points on graph with income gain on y-axis and age of move on x-axis for ages up to 28 with a downward slope from $38K to $23K at age 23, then levels off to 28.">
    <figcaption>Chetty and Hendren (2018).</figcaption>
</figure>

---
# One issue: families differ a ton

- Each family is different

1. Some families are rich, some are poor
2. Some families are more educated, some are less educated
3. Some families are religious, some are not

- Each of these differences could affect:

1. If they move
2. When they move
3. Where they move
4. Children's income mobility
5. Much more...

- We can't possible control for all of this, let alone measures some of it

---
# Fixed effects! 

- Fixed effects are a way to control for .pink[unobserved] variables that are .pink[constant] along some dimension
  - This dimension could be time, space, individual, etc.

- Fixed effects remove the variation between units, leaving only the variation within units

- Chetty and Hendren (2018) employ fixed effects to isolate .pink[within]-family variation
   - They do many other things too, but this is the simplest to folow

- Every family has its own unique characteristics that affect children's outcomes
- Fixed effects control for these characteristics by effectively removing the within-family mean from each observation

---
# Simplified dataset of mobility

- Let's look at a hypothetical dataframe for 5 families with 5 children each that move from the same low mobility neighborhood to a high mobility neighborhood

```{r family-creation, echo=FALSE}
set.seed(123)
num_families <- 5
num_children_per_family <- 5

df <- expand.grid(
  family_id = 1:num_families,
  child_id = 1:num_children_per_family
) %>%
group_by(family_id) %>%
mutate(
  average_age = round(runif(1, 4, 18),1),
  age_moved = average_age + sample(-4:4, n(), replace = FALSE),
  age_moved = round(ifelse(age_moved < 0, 0, age_moved),0),
  # Create a family effect that increases with average age
  family_effect = 2*average_age,
) %>%
ungroup() %>%
# Create random variable centered at average age running from 0 to 18
mutate(
  child_effect = rnorm(n(), mean = 0, sd = 1),
  income = 100 - .5*age_moved + family_effect + child_effect,
) %>%
group_by(family_id) %>%
mutate(family_label= case_when(income==max(income) ~
  paste("Family", family_id),TRUE ~ NA_character_),
  mean_income = mean(income),
  mean_age = mean(age_moved))

df %>% select(family_id,child_id,age_moved,income)
```

---
# Between and Within variation

- Below I plot the fake data

```{r between-within, echo=FALSE}
plot <- df %>% 
  ggplot(aes(x =  age_moved, y = income, label = family_label,color = factor(family_id))) + 
  geom_point() +
  geom_text(hjust = -.1, size = 14/.pt) + 
  theme_metro_regtitle() + 
  labs(x = 'Move age', 
       y = 'Adulthood income',
       caption = 'ChatGPT and GitHub copilot helped me create this.') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple','green'))

plot

```

---
# Between and Within

- If I just regress (pooled OLS), I get an increasing relationship with age moved! 

```{r with-ols, echo=FALSE}
plot +
  geom_smooth(method = 'lm', aes(color = NULL, label = NULL), se = FALSE) 
```

---
# Between and Within

- BETWEEN variation is the variation between means of each family

```{r with-between-ols, echo=FALSE}
plot+
  geom_point(aes(x = mean_age, y = mean_income, color = family_label), size = 30, shape = 3, color = 'darkorange')
  
```

---
# Between and Within

-Seriously, Only look at those means! The individual child variation within families does not matter

```{r between-only,echo=FALSE}
df %>% 
  ggplot(aes(x =  age_moved, y = income, label = family_label,color = factor(family_id))) + 
  theme_metro_regtitle() + 
  labs(x = 'Move age', 
       y = 'Adulthood income',
       caption = 'ChatGPT and GitHub copilot helped me create this.') + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple','green')) +
  geom_smooth(method = 'lm', aes(color = NULL, label = NULL), se = FALSE) +
  geom_point(aes(x = mean_age, y = mean_income, color = family_label), 
    size = 50, shape = 3, color = 'darkorange')
```

---
# Between and Within

- Within variation treats the orange crosses as their own axes and looks at variation within family
- We basically slide the axes on top of each other and analyze *that* data

```{r anim, echo=FALSE}
dfanim <- df %>%
  ungroup() %>%
  mutate(all_mean_income = mean(income),
         all_mean_age = mean(age_moved),
         stage = '1. Raw Data')
dfanim <- dfanim %>%
  bind_rows(dfanim %>% 
              mutate(income = income - mean_income + all_mean_income,
                     age_moved = age_moved - mean_age + all_mean_age,
                     mcrm = all_mean_income,
                     mpr = all_mean_age,
                     stage = '2. Remove all between variation'))

p <- ggplot(dfanim, aes(x =  age_moved, y = income, color = factor(family_id), label = family_label)) + 
  geom_point() + 
  geom_text(hjust = -.1, size = 14/.pt)  + 
  labs(x = 'Age of Move', 
       y = 'Income',
       caption = 'ChatGPT and GitHub copilot helped write this') + 
  #scale_x_continuous(limits = c(.15, 2.5)) + 
  guides(color = FALSE, label = FALSE) + 
  scale_color_manual(values = c('black','blue','red','purple','green')) +
  geom_smooth(aes(color = NULL), method = 'lm', se = FALSE)+
  geom_point(aes(x = mpr, y = mcrm), size = 20, shape = 3, color = 'darkorange') + 
  transition_states(stage) + 
  theme_metro_regtitle()

animate(p, nframes = 80)

```

---
# Removing between variation

- Chetty and Hendren (2018) use fixed effects to remove between-family variation
- But what does that mean?
- How do we actually do this?
  - Let's look at a stylized model

---
# Stylized model of Chetty and Hendren (2018)

$$
\begin{aligned}
  \text{Income}_{i} = \sum_{m=0}^{m=30} \beta_{m} I(\text{Move age}_{i}=m) + \epsilon_{i}
\end{aligned}
$$

but $\epsilon_i$ includes all that family variation, which is an omitted variable! That will create bias.

- We really have something like this:

$$
\begin{aligned}
  \text{Income}_{i} = \sum_{m=0}^{m=30} \beta_{m} I(\text{Move age}_{i}=m) + (\alpha_f + \nu_{i})
\end{aligned}
$$

$I()$ means "indicator function", which equals one when the Move age was $m$

---
# De-meaning

- Just like we de-meaned our plots, we can de-mean our data

- What happens if we subtract the mean of each variable for each family from each observation?
  - Well $\bar{\alpha}_f=\alpha_f$, so it is just gone! 

$$
\begin{aligned}
  \text{Income}_{i} - \bar{\text{Income}}_{i} = \sum_{m=0}^{m=30} \beta_{m} (I(\text{Move age}_{i}=m)-\bar{I(\text{Move age}_{i}=m)}) + \nu_{i}
\end{aligned}
$$

- This is called a .hi[fixed effect] model
- By construction, $\nu_i$ is no longer correlated with family characteristics

- Yes, I know that the average of $I(\text{Move age}_i=m)$ is a strange concept
  - Think of it as the probability of moving at age $m$ for each family
  - We benchmark the observed moving indicator on this family probability

- This works because of the Frisch-Waugh-Lovell Theorem (not derived in this class)

---
# # Fixed effects don't make Causality

- Fixed effects don't just make a regression causal
  - There may be other omitted variables that are correlated with the explanatory variable, not "absorbed" by your fixed effects

- All causal work requires assumptions

- .pink[Key assumption:] _timing_ of moves between areas is unrelated to other determinants of a child's outcomes

- Why might this not hold? 

--

  1. Parents who move to good areas when their children ar young might be different from those who move later 

  2. Moving may be related to other factors (e.g., change in parents' job) that affect children directly

---
# "Testing" assumptions

- You cannot fully test assumptions, but you can look for evidence they are violated

- Two approaches to evaluate validity of timing of move assumption:

  1. Compare siblings' outcomes to control for family "fixed" effects

  2. Use differences in neighborhood effects across subgroups to implement "placebo" tests

  - Ex: some places (e.g. low-crime areas) have better outcomes for boys than girls

  - Move to place where boys have higher earnings --> son improves in proportion to exposure, but not daughter

- Conclude that ~2/3 of variation in upward mobility across areas is due to causal effects of neighborhoods

---
# Fixed effects elsewhere

- Fixed effects are extremely popular in applied economics

- Any time you have panel data, you can bet a fixed effects model is attempted
  - Panel data is (usually) a dataset where you track a unit (individual, county, etc.) over time

- Plus, they form the backbone of difference-in-difference analysis

---
# Too many fixed effects?

- One challenge with fixed effects:
   - Each fixed effect is like adding a variable for each level of the fixed effect (each individual, each year, etc.)
   - This reduces the degrees of freedom in your model
   - As you add more fixed effects, you need more data to keep statistical power

The actual model in Chetty and Hendren (2018):

  $$
\begin{aligned}
  \text{Income}_{i} = \alpha_{qosm} + \sum_{m=1}^{m=30} b_m I(m_i=m) \Delta_{odps} + \sum_{s=1980}^{1987} \kappa_s I(s_i=s) \Delta_{odps} + \epsilon_{2i}
\end{aligned}
$$

has over 200,000 fixed effects!

---

# Limits to Fixed Effects

- Okay! At this point we have the concept behind fixed effects, can execute them, and know what they're good for
- What aren't they good for?

1. They don't control for anything that has within variation
2. They control away *everything* that's between-only, so we can't see the effect of anything that's between-only ("effect of a family on mobility?" Nope!)
3. Anything with only a *little* within variation will have most of its variation washed out too ("effect of population density on mobility?" probably not)
4. The estimate pays the most attention to individuals with *lots of variation in treatment*

- 2 and 3 can be addressed by using "random effects" instead but we aren't covering that in this class (see the The Effect chapter on Fixed Effects for more)

---

# Concept Checks

- Why can't we use individual-person fixed effects to study the impact of race on traffic stops?
- The within $R^2$ from is .3, and the overall $R^2$ is .5. Interpret these two numbers in sentences
- In a sentence, interpret the slope coefficient in the estimated model $(Y_{it} - \bar{Y}_i) = 1 + .5(X_{it} - \bar{X}_i)$ where $Y$ is "school funding per child" and $X$ is "population growth", and $i$ is city



## Dummy variables and interaction terms

For the next section, we'll need to create a dummy variable. We'll create two: one for whether a county is in the South and one for all major regions of the US. 

```{r in_south,message=F,warning=F}
# Get the regions and do minor cleaning
opp_atlas <- mutate(opp_atlas,
  in_south = ifelse(state_abb %in% c("AL", "AR", 
    "FL", "GA", "KY", "LA", "MS", "NC", "OK",
     "SC", "TN", "TX", "VA", "WV"), "South", "North"),
     positive_job_growth = ann_avg_job_growth_2004_2013>0) 
```

### Dummy variables as *factors*

Dummy variables are a core component of many regression models. However, these can be a pain to create in some statistical languages, since you first have to tabulate a whole new matrix of binary variables and then append it to the original data frame. In contrast, R has a very convenient framework for creating and evaluating dummy variables in a regression: Simply specify the variable of interest as a [factor](https://r4ds.had.co.nz/factors.html).^[Factors are variables that have distinct qualitative levels, e.g. "male", "female", "non-binary", etc.]

Here's an example where we explicitly tell R that "in_south" is a factor. Since I don't plan on reusing this model, I'm just going to print the results to screen rather than saving it to my global environment.

```{r ols_dv}
summary(feols(kfr_p25 ~ poor_share1990 + as.factor(in_south), data = opp_atlas))
```

Okay, I should tell you that I'm actually making things more complicated than they need to be with the heavy-handed emphasis on factors. R is "friendly" and tries to help whenever it thinks you have misspecified a function or variable. While this is something to be [aware of](https://rawgit.com/grantmcdermott/R-intro/master/rIntro.html#r_tries_to_guess_what_you_meant), normally It Just Works^TM^. A case in point is that we don't actually *need* to specify a string (i.e. character) variable as a factor in a regression. R will automatically do this for you regardless, since it's the only sensible way to include string variables in a regression.

```{r ols_dv2}
## Use the non-factored version of "in_south" instead; R knows it must be ordered
## for it to be included as a regression variable
summary(feols(kfr_p25 ~ poor_share1990 + in_south, data = opp_atlas))
```

What happens if I use region? It thinks region is a numeric variable and so it does not create dummy variables for it. 

```{r ols_dv3}
summary(feols(kfr_p25 ~ poor_share1990 + positive_job_growth, data = opp_atlas))
```

We can fix this by telling R that region is a factor.

```{r ols_dv4}
summary(feols(kfr_p25 ~ poor_share1990 + as.factor(region), data = opp_atlas))
```

### Interaction effects

Like dummy variables, R provides a convenient syntax for specifying interaction terms directly in the regression model without having to create them manually beforehand.^[Although there are very good reasons that you might want to modify your parent variables before doing so (e.g. centering them). As it happens, Grant McDermott [has strong feelings](https://twitter.com/grant_mcdermott/status/903691491414917122) that interaction effects are most widely misunderstood and misapplied concept in econometrics. However, that's a topic for another day.] You can use any of the following expansion operators:

- `x1:x2` "crosses" the variables (equivalent to including only the x1 × x2 interaction term)
- `x1/x2` "nests" the second variable within the first (equivalent to `x1 + x1:x2`; more on this [later](#nestedmarg))
- `x1*x2` includes all parent and interaction terms (equivalent to `x1 + x2 + x1:x2`) 

As a rule of thumb, if [not always](#nestedmarg), it is generally advisable to include all of the parent terms alongside their interactions. This makes the `*` option a good default. 

For example, we might wonder whether the relationship between a location's income mobility for children in the 25th percentile and its 1990 poverty rate differs by region. That is, we want to run a regression of the form,

$$KFR_{P25} = \beta_0 + \beta_1 D_{South} + \beta_2  + \beta_3 D_{South} \times 1990 Poverty Share$$

To implement this in R, we simply run the following,

```{r ols_ie}
ols_ie = feols(kfr_p25 ~ in_south * poor_share1990, data = opp_atlas)
summary(ols_ie)
```

## Marginal effects

Calculating marginal effects in a linear regression model like OLS is perfectly straightforward... just look at the coefficient values. But that quickly goes out the window when you have interaction terms or non-linear models like probit, logit, etc. Luckily, there are various ways to obtain these from R models. For example, we already saw the **mfx** package above for obtaining marginal effects from GLM models. I want to briefly focus on two of my favourite methods for obtaining marginal effects across different model classes: 1) The **margins** package and 2) a shortcut that works particularly well for models with interaction terms.

### The **margins** package

The **margins** package ([link](https://cran.r-project.org/web/packages/margins)), which is modeled on its namesake in Stata, is great for obtaining marginal effects across an entire range of models.^[I do, however, want to flag that it does [not yet support](https://github.com/leeper/margins/issues/128) **fixest** (or **lfe**) models. But there are [workarounds](https://github.com/leeper/margins/issues/128#issuecomment-636372023) in the meantime.] You can read more in the package [vignette](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html), but here's a very simple example to illustrate. 

Consider our interaction effects regression [from earlier](#interaction-effects), where we were interested in how income mobility varied by 1990 poverty rate and region. To get the average marginal effect (AME) of these dependent variables, we can just use the `margins::margins()` function.

```{r margins0, dependson="ols_ie"}
# library(margins) ## Already loaded

ols_ie_marg = margins(ols_ie)
```

Like a normal regression object, we can get a nice print-out display of the above object by summarising or tidying it.

```{r margins1, dependson="ols_ie"}
# summary(ols_ie_marg) ## Same effect
tidy(ols_ie_marg, conf.int = TRUE)
```

If we want to compare marginal effects at specific values --- e.g. how the AME of 1990 poverty rate on income mobility differs across north and south --- then that's easily done too.

```{r margins2, dependson="ols_ie"}
ols_ie %>% 
  margins(
    variables = "poor_share1990", ## The main variable we're interested in
    at = list(in_south = c("North","South")) ## How the main variable is modulated by at specific values of a second variable
    ) %>% 
  tidy(conf.int = TRUE) ## Tidy it (optional)
```

If you're the type of person who prefers visualizations (like me), then you should consider `margins::cplot()`, which is the package's in-built method for constructing *conditional* effect plots.

```{r margins3, dependson="ols_ie", dependson="opp_atlas"}
cplot(ols_ie, x = "in_south", dx = "poor_share1990", what = "effect", 
      data = opp_atlas)
```

In this case,it doesn't make much sense to read a lot into the larger standard errors on the female group; that's being driven by a very small sub-sample size.

Finally, you can also use `cplot()` to plot the predicted values of your outcome variable (here: "kfr_p25"), conditional on one of your dependent variables. For example:

```{r margins4, dependson="ols_ie", dependson="opp_atlas"}
par(mfrow=c(1, 2)) ## Just to plot these next two (base) figures side-by-side
cplot(ols_ie, x = "in_south", what = "prediction", data = opp_atlas)
cplot(ols_ie, x = "poor_share1990", what = "prediction", data = opp_atlas)
par(mfrow=c(1, 1)) ## Reset plot defaults
```

Note that `cplot()` uses the base R plotting method. If you'd prefer **ggplot2** equivalents, take a look at the **marginsplot** package ([link](https://github.com/vincentarelbundock/marginsplot)).

Finally, I also want to draw your attention to the **emmeans** package ([link](https://cran.r-project.org/web/packages/emmeans/index.html)), which provides very similar functionality to **margins**. I'm not as familiar with it myself, but I know that it has many fans.

### Special case: `/` shortcut for interaction terms {#nestedmarg}

I'll keep this one brief, but I wanted to mention one of my favourite R shortcuts: Obtaining the full marginal effects for interaction terms by using the `/` expansion operator. Grant [tweeted](https://twitter.com/grant_mcdermott/status/1202084676439085056?s=20) about this and even wrote an [whole blog post](https://grantmcdermott.com/2019/12/16/interaction-effects/) about it too (which you should totally read). But the very short version is that you can switch out the normal `f1 * x2` interaction terms syntax for `f1 / x2` and it automatically returns the full marginal effects. (The formal way to describe it is that the model variables have been "nested".)

Here's a super simple example, using the same interaction effects model from before.

```{r nested, dependson="opp_atlas"}
# ols_ie = lm(kfr_p25 ~ in_south * poor_share1990, data = opp_atlas) ## Original model
ols_ie_marg2 = lm(kfr_p25 ~ in_south / poor_share1990, data = opp_atlas)
tidy(ols_ie_marg2, conf.int = TRUE)
```

Note that the marginal effects on the two south × poverty rate interactions (i.e. `r round(ols_ie_marg2$coefficients[['in_southNorth:poor_share1990']], 3)` and `r round(ols_ie_marg2$coefficients[['in_southSouth:poor_share1990']], 3)`) are the same as we got with the `margins::margins()` function [above](#the-margins-package). 

Where this approach really shines is when you are estimating interaction terms in large models. The **margins** package relies on a numerical delta method which can be very computationally intensive, whereas using `/` adds no additional overhead beyond calculating the model itself. Still, that's about as much as say it here. Read the aforementioned [blog post](https://grantmcdermott.com/2019/12/16/interaction-effects/) if you'd like to learn more.
